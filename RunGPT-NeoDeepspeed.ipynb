{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models import gpt_neo\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import deepspeed\n",
    "import os\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9868' # modify if RuntimeError: Address already in use\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.distributed.init_process_group(backend=\"gloo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "deepspeed.init_distributed(\"gloo\")\n",
    "device = torch.device(\"cuda\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = gpt_neo.GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.requires_grad_(False)\n",
    "os.system(\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"train_batch_size\": 1,\n",
    "  \"zero_optimization\": {\n",
    "   \"stage\": 0,\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\"\n",
    "    },\n",
    "    \"offload_param\": {\n",
    "      \"device\": \"cpu\"\n",
    "    }\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": True\n",
    "  }\n",
    "}\n",
    "\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(model=model,\n",
    "                                                     config_params=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" This function has been mostly taken from huggingface conversational\n",
    "     ai code at\n",
    "         https://medium.com/huggingface/how-to-build-a-state-of-the-art-\n",
    "              conversational-ai-with-transfer-learning-2d818ac26313 \"\"\"\n",
    "\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the\n",
    "        # last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Cconvert to 1D\n",
    "        sorted_logits, sorted_indices = torch.sort(\n",
    "            logits, descending=True, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1),\n",
    "                                        dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token\n",
    "        # above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] \\\n",
    "            = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        for i in range(sorted_indices.size(0)):\n",
    "            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n",
    "            logits[i][indices_to_remove] = filter_value\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Deepspeed doesn't have a generate method, so i created mine.\n",
    "# Well... i just modified https://github.com/microsoft/DeepSpeedExamples/blob/127372571189ac905c8c92f4fe55a3d85c80324e/Megatron-LM-v1.1.5-3D_parallelism/megatron/text_generation_utils.py#L32\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             device,\n",
    "             tokens,\n",
    "             out_seq_length=50,\n",
    "             temperature=1,\n",
    "             top_k=0,\n",
    "             top_p=0):\n",
    "\n",
    "    tokens = tokens.input_ids.to(device)\n",
    "    context_length = len(tokens[0])\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    counter = 0\n",
    "\n",
    "    tokens = F.pad(tokens, (0,out_seq_length), value=2)\n",
    "\n",
    "    for i in range(out_seq_length):\n",
    "        logits = model(tokens).logits\n",
    "        logits = logits[:, context_length - 1, :] / temperature\n",
    "        logits = top_k_logits(logits, top_k=top_k, top_p=top_p)\n",
    "        log_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        prev = torch.multinomial(log_probs, num_samples=1)\n",
    "        tokens[0][context_length] = prev[0]\n",
    "        context_length += 1\n",
    "\n",
    "        if eos_id in tokens[0]:\n",
    "            break\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"You are Alex, a knight living in the kingdom of Larion. You have a steel longsword and a wooden shield. You are on a quest to defeat the evil dragon of Larion. You've heard he lives up at the north of the kingdom. You set on the path to defeat him and walk into a dark forest. As you enter the forest you see\"\n",
    "in_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "out_tokens = generate(model_engine, tokenizer, device, in_tokens, out_seq_length=50, top_k=60, top_p=0.9)[0]\n",
    "decoded = tokenizer.decode(out_tokens)\n",
    "print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
