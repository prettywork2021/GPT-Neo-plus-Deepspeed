{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models import gpt_neo\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import deepspeed\n",
    "import os\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9868' # modify if RuntimeError: Address already in use\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.distributed.init_process_group(backend=\"gloo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "deepspeed.init_distributed(\"gloo\")\n",
    "device = torch.device(\"cuda\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = gpt_neo.GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.requires_grad_(False)\n",
    "os.system(\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-05-03 21:20:04,914] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.3.15, git-hash=unknown, git-branch=unknown\n",
      "[2021-05-03 21:20:04,927] [INFO] [engine.py:80:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n",
      "[2021-05-03 21:20:05,855] [INFO] [config.py:741:print] DeepSpeedEngine configuration:\n",
      "[2021-05-03 21:20:05,856] [INFO] [config.py:745:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2021-05-03 21:20:05,856] [INFO] [config.py:745:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2021-05-03 21:20:05,858] [INFO] [config.py:745:print]   allreduce_always_fp32 ........ False\n",
      "[2021-05-03 21:20:05,859] [INFO] [config.py:745:print]   amp_enabled .................. False\n",
      "[2021-05-03 21:20:05,860] [INFO] [config.py:745:print]   amp_params ................... False\n",
      "[2021-05-03 21:20:05,861] [INFO] [config.py:745:print]   checkpoint_tag_validation_enabled  True\n",
      "[2021-05-03 21:20:05,862] [INFO] [config.py:745:print]   checkpoint_tag_validation_fail  False\n",
      "[2021-05-03 21:20:05,863] [INFO] [config.py:745:print]   disable_allgather ............ False\n",
      "[2021-05-03 21:20:05,864] [INFO] [config.py:745:print]   dump_state ................... False\n",
      "[2021-05-03 21:20:05,865] [INFO] [config.py:745:print]   dynamic_loss_scale_args ...... None\n",
      "[2021-05-03 21:20:05,866] [INFO] [config.py:745:print]   elasticity_enabled ........... False\n",
      "[2021-05-03 21:20:05,867] [INFO] [config.py:745:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 3, \n",
      "    \"detailed\": true\n",
      "}\n",
      "[2021-05-03 21:20:05,869] [INFO] [config.py:745:print]   fp16_enabled ................. True\n",
      "[2021-05-03 21:20:05,870] [INFO] [config.py:745:print]   global_rank .................. 0\n",
      "[2021-05-03 21:20:05,872] [INFO] [config.py:745:print]   gradient_accumulation_steps .. 1\n",
      "[2021-05-03 21:20:05,873] [INFO] [config.py:745:print]   gradient_clipping ............ 0.0\n",
      "[2021-05-03 21:20:05,874] [INFO] [config.py:745:print]   gradient_predivide_factor .... 1.0\n",
      "[2021-05-03 21:20:05,874] [INFO] [config.py:745:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2021-05-03 21:20:05,877] [INFO] [config.py:745:print]   loss_scale ................... 0\n",
      "[2021-05-03 21:20:05,877] [INFO] [config.py:745:print]   memory_breakdown ............. False\n",
      "[2021-05-03 21:20:05,879] [INFO] [config.py:745:print]   optimizer_legacy_fusion ...... False\n",
      "[2021-05-03 21:20:05,881] [INFO] [config.py:745:print]   optimizer_name ............... None\n",
      "[2021-05-03 21:20:05,882] [INFO] [config.py:745:print]   optimizer_params ............. None\n",
      "[2021-05-03 21:20:05,882] [INFO] [config.py:745:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2021-05-03 21:20:05,882] [INFO] [config.py:745:print]   pld_enabled .................. False\n",
      "[2021-05-03 21:20:05,883] [INFO] [config.py:745:print]   pld_params ................... False\n",
      "[2021-05-03 21:20:05,885] [INFO] [config.py:745:print]   prescale_gradients ........... False\n",
      "[2021-05-03 21:20:05,887] [INFO] [config.py:745:print]   scheduler_name ............... None\n",
      "[2021-05-03 21:20:05,888] [INFO] [config.py:745:print]   scheduler_params ............. None\n",
      "[2021-05-03 21:20:05,889] [INFO] [config.py:745:print]   sparse_attention ............. None\n",
      "[2021-05-03 21:20:05,891] [INFO] [config.py:745:print]   sparse_gradients_enabled ..... False\n",
      "[2021-05-03 21:20:05,893] [INFO] [config.py:745:print]   steps_per_print .............. 10\n",
      "[2021-05-03 21:20:05,894] [INFO] [config.py:745:print]   tensorboard_enabled .......... False\n",
      "[2021-05-03 21:20:05,895] [INFO] [config.py:745:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2021-05-03 21:20:05,896] [INFO] [config.py:745:print]   tensorboard_output_path ...... \n",
      "[2021-05-03 21:20:05,897] [INFO] [config.py:745:print]   train_batch_size ............. 1\n",
      "[2021-05-03 21:20:05,898] [INFO] [config.py:745:print]   train_micro_batch_size_per_gpu  1\n",
      "[2021-05-03 21:20:05,900] [INFO] [config.py:745:print]   wall_clock_breakdown ......... False\n",
      "[2021-05-03 21:20:05,900] [INFO] [config.py:745:print]   world_size ................... 1\n",
      "[2021-05-03 21:20:05,901] [INFO] [config.py:745:print]   zero_allow_untested_optimizer  False\n",
      "[2021-05-03 21:20:05,903] [INFO] [config.py:745:print]   zero_config .................. {\n",
      "    \"stage\": 0, \n",
      "    \"contiguous_gradients\": false, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+08, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": false, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": true, \n",
      "    \"offload_param\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 5, \n",
      "        \"buffer_size\": 1.000000e+08, \n",
      "        \"max_in_cpu\": 1.000000e+09, \n",
      "        \"pin_memory\": false\n",
      "    }, \n",
      "    \"offload_optimizer\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 4, \n",
      "        \"pin_memory\": false, \n",
      "        \"pipeline_read\": false, \n",
      "        \"pipeline_write\": false, \n",
      "        \"fast_init\": false, \n",
      "        \"pipeline\": false\n",
      "    }, \n",
      "    \"sub_group_size\": 1.000000e+12, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_fp16_weights_on_model_save\": false\n",
      "}\n",
      "[2021-05-03 21:20:05,906] [INFO] [config.py:745:print]   zero_enabled ................. False\n",
      "[2021-05-03 21:20:05,908] [INFO] [config.py:745:print]   zero_optimization_stage ...... 0\n",
      "[2021-05-03 21:20:05,908] [INFO] [config.py:752:print]   json = {\n",
      "    \"train_batch_size\": 1, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }\n",
      "}\n",
      "Using C:\\Users\\SpaceEye\\AppData\\Local\\torch_extensions\\torch_extensions\\Cache as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000997304916381836 seconds\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "  \"train_batch_size\": 1,\n",
    "  \"zero_optimization\": {\n",
    "   \"stage\": 0,\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\"\n",
    "    },\n",
    "    \"offload_param\": {\n",
    "      \"device\": \"cpu\"\n",
    "    }\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": True\n",
    "  }\n",
    "}\n",
    "\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(model=model,\n",
    "                                                     config_params=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" This function has been mostly taken from huggingface conversational\n",
    "     ai code at\n",
    "         https://medium.com/huggingface/how-to-build-a-state-of-the-art-\n",
    "              conversational-ai-with-transfer-learning-2d818ac26313 \"\"\"\n",
    "\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the\n",
    "        # last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Cconvert to 1D\n",
    "        sorted_logits, sorted_indices = torch.sort(\n",
    "            logits, descending=True, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1),\n",
    "                                        dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token\n",
    "        # above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] \\\n",
    "            = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        for i in range(sorted_indices.size(0)):\n",
    "            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n",
    "            logits[i][indices_to_remove] = filter_value\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Deepspeed doesn't have a generate method, so i created mine.\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             device,\n",
    "             tokens,\n",
    "             out_seq_length=50,\n",
    "             temperature=1,\n",
    "             top_k=0,\n",
    "             top_p=0):\n",
    "\n",
    "    tokens = tokens.input_ids.to(device)\n",
    "    context_length = len(tokens[0])\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    counter = 0\n",
    "\n",
    "    tokens = F.pad(tokens, (0,out_seq_length), value=2)\n",
    "\n",
    "    while counter < out_seq_length:\n",
    "        logits = model(tokens).logits\n",
    "        logits = logits[:, context_length - 1, :] / temperature\n",
    "        logits = top_k_logits(logits, top_k=top_k, top_p=top_p)\n",
    "        log_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        prev = torch.multinomial(log_probs, num_samples=1)\n",
    "        tokens[0][context_length] = prev[0]\n",
    "        context_length += 1\n",
    "        counter += 1\n",
    "\n",
    "        if eos_id in tokens[0]:\n",
    "            break\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Alex, a knight living in the kingdom of Larion. You have a steel longsword and a wooden shield. You are on a quest to defeat the evil dragon of Larion. You've heard he lives up at the north of the kingdom. You set on the path to defeat him and walk into a dark forest. As you enter the forest you see a strange man, wearing a red cape. He looks like he is in his 70's Jet Black. The man begins talking to you. He says his name is John (from his lips to yours, you get the name \"Alex\" and \"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are Alex, a knight living in the kingdom of Larion. You have a steel longsword and a wooden shield. You are on a quest to defeat the evil dragon of Larion. You've heard he lives up at the north of the kingdom. You set on the path to defeat him and walk into a dark forest. As you enter the forest you see\"\n",
    "in_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "out_tokens = generate(model_engine, tokenizer, device, in_tokens, out_seq_length=50, top_k=60, top_p=0.9)[0]\n",
    "decoded = tokenizer.decode(out_tokens)\n",
    "print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}