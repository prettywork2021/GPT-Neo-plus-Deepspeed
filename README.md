# GPT-Neo-plus-Deepspeed

I have tried to use deepspeed zero offloading (https://www.deepspeed.ai/tutorials/zero-offload/) to run GPT-Neo with small VRAM gpu's. Sadly, it didn't work. Either it is impossible, or i did something wrong. My previous tests, were incorrect. I may try other methods in the future, but for now my solution doesn't work. ¯\\_(ツ)_/¯

If you want to play with GPT-Neo, you can use https://colab.research.google.com/github/finetuneanon/gpt-neo_dungeon/blob/master/gpt-neo_dungeon.ipynb
